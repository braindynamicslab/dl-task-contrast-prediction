{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation Plots\n",
    "\n",
    "Includes code and results for various correlation related results like:\n",
    "- Single model correlation plots (within and across subject)\n",
    "- Model comparison via correlation\n",
    "- Subject identification accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import nilearn as nil\n",
    "import neuromaps as nm\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "from utils.utilities import CONTRASTS, GROUP_CONTRAST_IDS, plot_corr_matrices_across_contrasts, compute_corr_coeff, scale\n",
    "\n",
    "base_data_dir = '../../data/'\n",
    "mask = np.load('../data/glasser_medial_wall_mask.npy')\n",
    "paper_subset_indicies = np.array([2,3,4,6,7,9,10,12,13,14,15,16,18,19,20,21,23,24,25,26,31,38,44,45])\n",
    "paper_subset_contrasts = [CONTRASTS[idx] for idx in paper_subset_indicies]\n",
    "\n",
    "test_subj_ids = np.genfromtxt(\"../data/MICCAI2020/HCP_test_retest_subj_ids.csv\", dtype='<U13')\n",
    "\n",
    "test_contrasts = []\n",
    "for i in range(len(test_subj_ids)):\n",
    "    subj = test_subj_ids[i]\n",
    "    contrast_file = os.path.join(base_data_dir, \"test_contrasts\", \"%s_joint_LR_task_contrasts.npy\" % subj)\n",
    "    contrast_data = np.load(contrast_file)\n",
    "    test_contrasts.append(contrast_data)\n",
    "\n",
    "test_contrasts = np.asarray(test_contrasts)\n",
    "test_contrasts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_comparison_correlation(pred_by_model, colors=[ '#f99154', '#63bfa6', '#358cbb', '#3aac11', '#a89154','#e0d57e'], metric='Correlation with Groundtruth', indicies=np.arange(len(CONTRASTS)), legend_size=24, ymax=0.901, ymin=-0.1):\n",
    "    colors = colors[:len(pred_by_model.keys())]\n",
    "    sns.set_palette(sns.color_palette(colors))\n",
    "    # metric = 'Correlation Increase of Self vs Mean Others'\n",
    "    df = pd.DataFrame(columns=[\"Model\", \"Task Contrast\", \"Subject\", \"Correlation with Groundtruth\", \"Correlation Difference\", \"Rank\"])\n",
    "    for model in pred_by_model:\n",
    "        all_subj_contrast_corr = pred_by_model[model]\n",
    "        for i in indicies:\n",
    "            item = CONTRASTS[i]\n",
    "            task, cope_id, contrast_label = item\n",
    "            key = \"%s %s\" % (task, contrast_label)\n",
    "\n",
    "            contrast_corr = all_subj_contrast_corr[i]\n",
    "\n",
    "            count = 0\n",
    "            for j in range(len(test_subj_ids)):\n",
    "                count = count  + 1\n",
    "                corr_row = contrast_corr[j, :]\n",
    "                self_corr = corr_row[j]\n",
    "                other_corrs = np.concatenate((corr_row[:j], corr_row[j+1:]))\n",
    "                mean_other_corr = np.mean(other_corrs)\n",
    "\n",
    "                num_other_gt_self = np.sum(other_corrs > self_corr)\n",
    "                sorted_indices = np.flip(np.argsort(corr_row))\n",
    "                for k in range(len(sorted_indices)):\n",
    "                    if sorted_indices[k] == j:\n",
    "                        rank = k\n",
    "                rank = rank + 1\n",
    "                df.loc[len(df.index)] = [model,key,test_subj_ids[j],self_corr,(self_corr - mean_other_corr),rank]\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(36, 10))\n",
    "    sns.boxplot(x=\"Task Contrast\",\n",
    "                y=metric, hue=\"Model\",\n",
    "                data=df, ax=ax, # palette=\"Set3\",\n",
    "                hue_order=list(pred_by_model.keys()))\n",
    "    sns.stripplot(x=\"Task Contrast\",\n",
    "                y=metric, hue=\"Model\",\n",
    "                data=df, ax=ax, # palette=\"Set3\",\n",
    "                hue_order=list(pred_by_model.keys()),\n",
    "                dodge=True, legend=False)\n",
    "    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\") \n",
    "    L = ax.legend(frameon=False, ncol=len(pred_by_model.keys()), loc='upper center', bbox_to_anchor=(0.5, 1.1), fontsize=legend_size)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.set_ylim(ymin, ymax)\n",
    "    ax.set_yticks(np.arange(ymin, ymax, 0.2))\n",
    "    ax.set_xlim(-1, len(indicies))\n",
    "    ax.tick_params(direction=\"in\", labelsize=24)\n",
    "    ax.xaxis.get_label().set_fontsize(40)\n",
    "    ax.yaxis.get_label().set_fontsize(40)\n",
    "    ax.tick_params(length = 10)\n",
    "    plt.show()\n",
    "\n",
    "def plot_model_comparison_correlation_w_performance(pred_by_model, performance, contrasts, colors=[ '#f99154', '#63bfa6', '#358cbb', '#3aac11', '#a89154','#e0d57e'], metric='Correlation with Groundtruth', indicies=np.arange(len(CONTRASTS)), legend_size=24):\n",
    "    colors = colors[:len(pred_by_model.keys())]\n",
    "    sns.set_palette(sns.color_palette(colors))\n",
    "    \n",
    "    df = pd.DataFrame(columns=[\"Model\", \"Task Contrast\", \"Subject\", \"Correlation with Groundtruth\", \"Correlation Increase of Self vs Mean Others\", \"Performance\", \"Rank\"])\n",
    "    for model in pred_by_model:\n",
    "        all_subj_contrast_corr = pred_by_model[model]\n",
    "        for i in indicies:\n",
    "            item = contrasts[i]\n",
    "            task, cope_id, contrast_label = item\n",
    "            key = \"%s %s\" % (task, contrast_label)\n",
    "\n",
    "            contrast_corr = all_subj_contrast_corr[i]\n",
    "            task_performance = performance[i]\n",
    "\n",
    "            count = 0\n",
    "            for j in range(len(test_subj_ids)):\n",
    "                count = count  + 1\n",
    "                corr_row = contrast_corr[j, :]\n",
    "                self_corr = corr_row[j]\n",
    "                other_corrs = np.concatenate((corr_row[:j], corr_row[j+1:]))\n",
    "                mean_other_corr = np.mean(other_corrs)\n",
    "\n",
    "                num_other_gt_self = np.sum(other_corrs > self_corr)\n",
    "                sorted_indices = np.flip(np.argsort(corr_row))\n",
    "                for k in range(len(sorted_indices)):\n",
    "                    if sorted_indices[k] == j:\n",
    "                        rank = k\n",
    "                rank = rank + 1\n",
    "                subj_perf = 'Above' if task_performance[j] > np.mean(task_performance) else 'Below'\n",
    "                df.loc[len(df.index)] = [model,key,test_subj_ids[j],self_corr,(self_corr - mean_other_corr), subj_perf, rank]\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(36, 10))\n",
    "    sns.boxplot(x=\"Task Contrast\",\n",
    "                y=metric, hue=\"Model\",\n",
    "                data=df, ax=ax, # palette=\"Set3\",\n",
    "                hue_order=list(pred_by_model.keys()))\n",
    "    sns.stripplot(x=\"Task Contrast\",\n",
    "                y=\"Performance\", hue=\"Model\", hue_order=['Above', 'Below'],\n",
    "                data=df, ax=ax, palette=sns.color_palette(\"vlag\", as_cmap=True),\n",
    "                dodge=True, legend=True)\n",
    "    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\") \n",
    "    L = ax.legend(frameon=False, ncol=len(pred_by_model.keys()), loc='upper center', bbox_to_anchor=(0.5, 1.1), fontsize=legend_size)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.set_ylim(-0.1, 0.901)\n",
    "    ax.set_yticks(np.arange(-0.1, 0.901, 0.2))\n",
    "    ax.set_xlim(-1, len(indicies))\n",
    "    ax.tick_params(direction=\"in\", labelsize=24)\n",
    "    ax.xaxis.get_label().set_fontsize(40)\n",
    "    ax.yaxis.get_label().set_fontsize(40)\n",
    "    ax.tick_params(length = 10)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_subj_contrast_corr(pred, ref, contrasts, contrast_ids, mask):\n",
    "    all_lh_corr = []\n",
    "    all_rh_corr = []\n",
    "    all_avg_corr = []\n",
    "    \n",
    "    masked_lh_pred = pred[:, ::2, mask[0, :]]\n",
    "    masked_rh_pred = pred[:, 1::2, mask[1, :]]\n",
    "    \n",
    "    masked_lh_ref = ref[:, ::2, mask[0, :]]\n",
    "    masked_rh_ref = ref[:, 1::2, mask[1, :]]\n",
    "\n",
    "    for i in range(len(contrasts)):\n",
    "        lh_contrast_ref = masked_lh_ref[:, i, :]\n",
    "        rh_contrast_ref = masked_rh_ref[:, i, :]\n",
    "\n",
    "        lh_contrast_pred = masked_lh_pred[:, i, :]\n",
    "        rh_contrast_pred = masked_rh_pred[:, i, :]\n",
    "\n",
    "        lh_corr = compute_corr_coeff(lh_contrast_ref, lh_contrast_pred)\n",
    "        # print(compute_corr_coeff(lh_contrast_ref, lh_contrast_pred) == compute_corr_coeff(lh_contrast_pred, lh_contrast_ref))\n",
    "        rh_corr = compute_corr_coeff(rh_contrast_ref, rh_contrast_pred)\n",
    "        # print(lh_corr[0, :] - lh_corr[1, :])\n",
    "\n",
    "        all_lh_corr.append(lh_corr)\n",
    "        all_rh_corr.append(rh_corr)\n",
    "        all_avg_corr.append((lh_corr + rh_corr) / 2)\n",
    "    return all_lh_corr, all_rh_corr, all_avg_corr\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retest Contrasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retest = {}\n",
    "for ic in [1]:\n",
    "   print('--------------------------------------------------')\n",
    "   label = f'Retest Contrasts'\n",
    "   print(label)\n",
    "   mse_brainsurf_path = f\"../../data/retest_contrasts/contrasts/\"\n",
    "\n",
    "   multisample_brainsurfcnn_mse_pred = []\n",
    "   for i in range(len(test_subj_ids)):\n",
    "      subj = test_subj_ids[i]\n",
    "      pred_file = os.path.join(mse_brainsurf_path, \"%s_joint_LR_task_contrasts.npy\" % subj)\n",
    "      pred = np.load(pred_file)\n",
    "      multisample_brainsurfcnn_mse_pred.append(pred)\n",
    "\n",
    "   brainsurfcnn_mse_pred = np.asarray(multisample_brainsurfcnn_mse_pred)\n",
    "   \n",
    "   _, _, test_brainsurfcnn_mse_corr = compute_subj_contrast_corr(test_contrasts, brainsurfcnn_mse_pred, CONTRASTS, GROUP_CONTRAST_IDS, mask)\n",
    "\n",
    "   paper_subset_mse_corr = [test_brainsurfcnn_mse_corr[idx] for idx in paper_subset_indicies]\n",
    "\n",
    "   total = 0\n",
    "   correct = 0\n",
    "   accs = []\n",
    "   for matrix, contrast in zip(paper_subset_mse_corr, paper_subset_contrasts):\n",
    "      matrix = scale(matrix, axis=0)\n",
    "      matrix = matrix.T\n",
    "      for i, row in enumerate(matrix):\n",
    "         total += 1\n",
    "         correct += int(np.argmax(row) == i)\n",
    "      accs.append(correct/total * 100.0)\n",
    "      correct = 0\n",
    "      total = 0\n",
    "\n",
    "   retest[label] = test_brainsurfcnn_mse_corr   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BrainSurf CNN - MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brainsurf_mse_models = {}\n",
    "for ic in [15, 25, 50, 100]:\n",
    "   print('--------------------------------------------------')\n",
    "   label = f'BrainSurfCNN: {ic} ICS - MSE'\n",
    "   print(f'BrainSurfCNN: {ic} ICS - MSE')\n",
    "   mse_brainsurf_path = f\"../../aim3_results/HCP_feat64_s8_c{ic}_lr0.01_seed28_epochs50/hcp_{ic}_sample8_feat64_s8_c{ic}_lr0.01_seed28/predict_on_test_subj/best_corr/\"\n",
    "\n",
    "   multisample_brainsurfcnn_mse_pred = []\n",
    "   for i in range(len(test_subj_ids)):\n",
    "      subj = test_subj_ids[i]\n",
    "      pred_file = os.path.join(mse_brainsurf_path, \"%s_pred.npy\" % subj)\n",
    "      pred = np.load(pred_file)\n",
    "      multisample_brainsurfcnn_mse_pred.append(pred)\n",
    "\n",
    "   multisample_brainsurfcnn_mse_pred = np.asarray(multisample_brainsurfcnn_mse_pred)\n",
    "   brainsurfcnn_mse_pred = np.mean(multisample_brainsurfcnn_mse_pred, 1)\n",
    "\n",
    "   _, _, test_brainsurfcnn_mse_corr = compute_subj_contrast_corr(test_contrasts, brainsurfcnn_mse_pred, CONTRASTS, GROUP_CONTRAST_IDS, mask)\n",
    "\n",
    "   plot_corr_matrices_across_contrasts(test_brainsurfcnn_mse_corr[0:3], CONTRASTS[0:3], vmin=0.35 , vmax=0.9)\n",
    "   plt.show()\n",
    "   plot_corr_matrices_across_contrasts([test_brainsurfcnn_mse_corr[7], test_brainsurfcnn_mse_corr[4], test_brainsurfcnn_mse_corr[12]], [CONTRASTS[7], CONTRASTS[4], CONTRASTS[12]], vmin=0.35 , vmax=0.9)\n",
    "   plt.show()\n",
    "\n",
    "   paper_subset_mse_corr = [test_brainsurfcnn_mse_corr[idx] for idx in paper_subset_indicies]\n",
    "\n",
    "   total = 0\n",
    "   correct = 0\n",
    "   accs = []\n",
    "   for matrix, contrast in zip(test_brainsurfcnn_mse_corr, CONTRASTS):\n",
    "      matrix = scale(matrix, axis=0)\n",
    "      matrix = matrix.T\n",
    "      for i, row in enumerate(matrix):\n",
    "         total += 1\n",
    "         correct += int(np.argmax(row) == i)\n",
    "      accs.append(correct/total * 100.0)\n",
    "      correct = 0\n",
    "      total = 0\n",
    "\n",
    "   brainsurf_mse_models[label] = test_brainsurfcnn_mse_corr   \n",
    "   print('Max ACC:', CONTRASTS[np.argmax(accs)], np.max(accs))\n",
    "   print('Min ACC:', CONTRASTS[np.argmin(accs)], np.min(accs))\n",
    "   print('Avg ACC:',np.mean(accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_comparison_correlation(pred_by_model=brainsurf_mse_models)\n",
    "plot_model_comparison_accuracy(pred_by_model=brainsurf_mse_models, indicies=paper_subset_indicies)\n",
    "plot_model_comparison_accuracy(pred_by_model=brainsurf_mse_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagged 5-Fold Cross Validation - MSE\n",
    "25 ICs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brainsurf_cv_models = {}\n",
    "ic = 25\n",
    "all_fold_predictions = []\n",
    "for fold in range(6):\n",
    "   print('--------------------------------------------------')\n",
    "   if fold < 5:\n",
    "      label = f'BaggedSurfCNN: {ic} ICS - Fold {fold+1}'\n",
    "      print(label)\n",
    "      cv_brainsurf_path = f\"../../brainsurf_model/HCP_feat64_s8_c{ic}_lr0.01_seed28_epochs50/kfold_{ic}_sample8/predict_on_test_subj_{fold}/best_corr/\"\n",
    "\n",
    "      multisample_brainsurfcnn_ft_pred = []\n",
    "      for i in range(len(test_subj_ids)):\n",
    "         subj = test_subj_ids[i]\n",
    "         pred_file = os.path.join(cv_brainsurf_path, \"%s_pred.npy\" % subj)\n",
    "         pred = np.load(pred_file)\n",
    "         multisample_brainsurfcnn_ft_pred.append(pred)\n",
    "         \n",
    "      multisample_brainsurfcnn_ft_pred = np.asarray(multisample_brainsurfcnn_ft_pred)\n",
    "      all_fold_predictions.append(multisample_brainsurfcnn_ft_pred)\n",
    "\n",
    "   else:\n",
    "      label = f'BaggedSurfCNN: {ic} ICS - Bagged'\n",
    "      print(label)\n",
    "      multisample_brainsurfcnn_ft_pred = np.mean(all_fold_predictions, 0)\n",
    "\n",
    "   \n",
    "   brainsurfcnn_mse_pred = np.mean(multisample_brainsurfcnn_ft_pred, 1)\n",
    "\n",
    "   _, _, test_brainsurfcnn_mse_corr = compute_subj_contrast_corr(test_contrasts, brainsurfcnn_mse_pred, CONTRASTS, GROUP_CONTRAST_IDS, mask)\n",
    "\n",
    "   plot_corr_matrices_across_contrasts(test_brainsurfcnn_mse_corr[0:3], CONTRASTS[0:3], vmin=0.35 , vmax=0.9)\n",
    "   plt.show()\n",
    "   plot_corr_matrices_across_contrasts([test_brainsurfcnn_mse_corr[7], test_brainsurfcnn_mse_corr[4], test_brainsurfcnn_mse_corr[12]], [CONTRASTS[7], CONTRASTS[4], CONTRASTS[12]], vmin=0.35 , vmax=0.9)\n",
    "   plt.show()\n",
    "\n",
    "   paper_subset_mse_corr = [test_brainsurfcnn_mse_corr[idx] for idx in paper_subset_indicies]\n",
    "\n",
    "   total = 0\n",
    "   correct = 0\n",
    "   accs = []\n",
    "   for matrix, contrast in zip(test_brainsurfcnn_mse_corr, CONTRASTS):\n",
    "      matrix = scale(matrix, axis=0)\n",
    "      matrix = matrix.T\n",
    "      for i, row in enumerate(matrix):\n",
    "         total += 1\n",
    "         correct += int(np.argmax(row) == i)\n",
    "      accs.append(correct/total * 100.0)\n",
    "      correct = 0\n",
    "      total = 0\n",
    "\n",
    "   brainsurf_cv_models[label] = test_brainsurfcnn_mse_corr  \n",
    "   \n",
    "   print('Max ACC:', CONTRASTS[np.argmax(accs)], np.max(accs))\n",
    "   print('Min ACC:', CONTRASTS[np.argmin(accs)], np.min(accs))\n",
    "   print('Avg ACC:',np.mean(accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_comparison_correlation(pred_by_model=brainsurf_cv_models, legend_size=18)\n",
    "plot_model_comparison_accuracy(pred_by_model=brainsurf_cv_models, indicies=paper_subset_indicies, legend_size=12)\n",
    "plot_model_comparison_accuracy(pred_by_model=brainsurf_cv_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BrainSurfATN - MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brainsurfatn_mse_models = {}\n",
    "for ic in [15, 25, 50, 100]:\n",
    "   print('--------------------------------------------------')\n",
    "   label = f'BrainSurfATN: {ic} ICS - MSE'\n",
    "   print(label)\n",
    "   mse_brainsurf_path = f\"../../brainsurf_model/HCP_feat64_s8_c{ic}_lr0.01_seed28_epochs50/attn_{ic}_sample8_feat64_s8_c{ic}_lr0.01_seed28/predict_on_test_subj/best_corr/\"\n",
    "\n",
    "   multisample_brainsurfcnn_mse_pred = []\n",
    "   for i in range(len(test_subj_ids)):\n",
    "      subj = test_subj_ids[i]\n",
    "      pred_file = os.path.join(mse_brainsurf_path, \"%s_pred.npy\" % subj)\n",
    "      pred = np.load(pred_file)\n",
    "      multisample_brainsurfcnn_mse_pred.append(pred)\n",
    "\n",
    "   multisample_brainsurfcnn_mse_pred = np.asarray(multisample_brainsurfcnn_mse_pred)\n",
    "   brainsurfcnn_mse_pred = np.mean(multisample_brainsurfcnn_mse_pred, 1)\n",
    "\n",
    "   _, _, test_brainsurfcnn_mse_corr = compute_subj_contrast_corr(test_contrasts, brainsurfcnn_mse_pred, CONTRASTS, GROUP_CONTRAST_IDS, mask)\n",
    "\n",
    "   plot_corr_matrices_across_contrasts(test_brainsurfcnn_mse_corr[0:3], CONTRASTS[0:3], vmin=0.35 , vmax=0.9)\n",
    "   plt.show()\n",
    "   plot_corr_matrices_across_contrasts([test_brainsurfcnn_mse_corr[7], test_brainsurfcnn_mse_corr[4], test_brainsurfcnn_mse_corr[12]], [CONTRASTS[7], CONTRASTS[4], CONTRASTS[12]], vmin=0.35 , vmax=0.9)\n",
    "   plt.show()\n",
    "\n",
    "   paper_subset_mse_corr = [test_brainsurfcnn_mse_corr[idx] for idx in paper_subset_indicies]\n",
    "\n",
    "   total = 0\n",
    "   correct = 0\n",
    "   accs = []\n",
    "   for matrix, contrast in zip(test_brainsurfcnn_mse_corr, CONTRASTS):\n",
    "      matrix = scale(matrix, axis=0)\n",
    "      matrix = matrix.T\n",
    "      for i, row in enumerate(matrix):\n",
    "         total += 1\n",
    "         correct += int(np.argmax(row) == i)\n",
    "      accs.append(correct/total * 100.0)\n",
    "      correct = 0\n",
    "      total = 0\n",
    "\n",
    "   brainsurfatn_mse_models[label] = test_brainsurfcnn_mse_corr   \n",
    "   print('Max ACC:', CONTRASTS[np.argmax(accs)], np.max(accs))\n",
    "   print('Min ACC:', CONTRASTS[np.argmin(accs)], np.min(accs))\n",
    "   print('Avg ACC:',np.mean(accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_comparison_correlation(pred_by_model=brainsurfatn_mse_models, legend_size=18)\n",
    "plot_model_comparison_accuracy(pred_by_model=brainsurfatn_mse_models, indicies=paper_subset_indicies, legend_size=12)\n",
    "plot_model_comparison_accuracy(pred_by_model=brainsurfatn_mse_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BrainSERF - MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brainserf_mse_models = {}\n",
    "for ic in [15, 25, 50]:\n",
    "   print('--------------------------------------------------')\n",
    "   label = f'BrainSERF: {ic} ICS - MSE'\n",
    "   print(f'BrainSERF: {ic} ICS - MSE')\n",
    "   mse_brainsurf_path = f\"../../aim3_results/HCP_feat64_s8_c{ic}_lr0.01_seed28_epochs50/se_attn_{ic}_sample8_feat64_s8_c{ic}_lr0.01_seed28/predict_on_test_subj/best_corr/\"\n",
    "\n",
    "   multisample_brainsurfcnn_mse_pred = []\n",
    "   for i in range(len(test_subj_ids)):\n",
    "      subj = test_subj_ids[i]\n",
    "      pred_file = os.path.join(mse_brainsurf_path, \"%s_pred.npy\" % subj)\n",
    "      pred = np.load(pred_file)\n",
    "      multisample_brainsurfcnn_mse_pred.append(pred)\n",
    "\n",
    "   multisample_brainsurfcnn_mse_pred = np.asarray(multisample_brainsurfcnn_mse_pred)\n",
    "   brainsurfcnn_mse_pred = np.mean(multisample_brainsurfcnn_mse_pred, 1)\n",
    "\n",
    "   _, _, test_brainsurfcnn_mse_corr = compute_subj_contrast_corr(test_contrasts, brainsurfcnn_mse_pred, CONTRASTS, GROUP_CONTRAST_IDS, mask)\n",
    "\n",
    "   plot_corr_matrices_across_contrasts(test_brainsurfcnn_mse_corr[0:3], CONTRASTS[0:3], vmin=0.35 , vmax=0.9)\n",
    "   plt.show()\n",
    "   plot_corr_matrices_across_contrasts([test_brainsurfcnn_mse_corr[7], test_brainsurfcnn_mse_corr[4], test_brainsurfcnn_mse_corr[12]], [CONTRASTS[7], CONTRASTS[4], CONTRASTS[12]], vmin=0.35 , vmax=0.9)\n",
    "   plt.show()\n",
    "\n",
    "   paper_subset_mse_corr = [test_brainsurfcnn_mse_corr[idx] for idx in paper_subset_indicies]\n",
    "\n",
    "   total = 0\n",
    "   correct = 0\n",
    "   accs = []\n",
    "   for matrix, contrast in zip(test_brainsurfcnn_mse_corr, CONTRASTS):\n",
    "      matrix = scale(matrix, axis=0)\n",
    "      matrix = matrix.T\n",
    "      for i, row in enumerate(matrix):\n",
    "         total += 1\n",
    "         correct += int(np.argmax(row) == i)\n",
    "      accs.append(correct/total * 100.0)\n",
    "      correct = 0\n",
    "      total = 0\n",
    "\n",
    "   brainserf_mse_models[label] = test_brainsurfcnn_mse_corr   \n",
    "   print('Max ACC:', CONTRASTS[np.argmax(accs)], np.max(accs))\n",
    "   print('Min ACC:', CONTRASTS[np.argmin(accs)], np.min(accs))\n",
    "   print('Avg ACC:',np.mean(accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_comparison_correlation(pred_by_model=brainserf_mse_models, legend_size=18)\n",
    "plot_model_comparison_accuracy(pred_by_model=brainserf_mse_models, indicies=paper_subset_indicies, legend_size=12)\n",
    "plot_model_comparison_accuracy(pred_by_model=brainserf_mse_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BrainSurfGNN - MSE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brainsurfgnn_mse_models = {}\n",
    "for ic in [15, 25, 50, 100]:\n",
    "   print('--------------------------------------------------')\n",
    "   label = f'BrainSurfGNN: {ic} ICS - MSE'\n",
    "   print(f'BrainSurfGNN: {ic} ICS - MSE')\n",
    "   mse_brainsurf_path = f\"../../aim3_results/HCP_feat64_s8_c{ic}_lr0.01_seed28_epochs50/gnn_mse_larger_feat64_s8_c{ic}_lr0.01_seed28/predict_on_test_subj/best_corr/\"\n",
    "\n",
    "   multisample_brainsurfcnn_mse_pred = []\n",
    "   for i in range(len(test_subj_ids)):\n",
    "      subj = test_subj_ids[i]\n",
    "      pred_file = os.path.join(mse_brainsurf_path, \"%s_pred.npy\" % subj)\n",
    "      pred = np.load(pred_file)\n",
    "      multisample_brainsurfcnn_mse_pred.append(pred)\n",
    "\n",
    "   multisample_brainsurfcnn_mse_pred = np.asarray(multisample_brainsurfcnn_mse_pred)\n",
    "   brainsurfcnn_mse_pred = np.mean(multisample_brainsurfcnn_mse_pred, 1)\n",
    "\n",
    "   _, _, test_brainsurfcnn_mse_corr = compute_subj_contrast_corr(test_contrasts, brainsurfcnn_mse_pred, CONTRASTS, GROUP_CONTRAST_IDS, mask)\n",
    "\n",
    "   plot_corr_matrices_across_contrasts(test_brainsurfcnn_mse_corr[0:3], CONTRASTS[0:3], vmin=0.35 , vmax=0.9)\n",
    "   plt.show()\n",
    "   plot_corr_matrices_across_contrasts([test_brainsurfcnn_mse_corr[7], test_brainsurfcnn_mse_corr[4], test_brainsurfcnn_mse_corr[12]], [CONTRASTS[7], CONTRASTS[4], CONTRASTS[12]], vmin=0.35 , vmax=0.9)\n",
    "   plt.show()\n",
    "\n",
    "   paper_subset_mse_corr = [test_brainsurfcnn_mse_corr[idx] for idx in paper_subset_indicies]\n",
    "\n",
    "   total = 0\n",
    "   correct = 0\n",
    "   accs = []\n",
    "   for matrix, contrast in zip(test_brainsurfcnn_mse_corr, CONTRASTS):\n",
    "      matrix = scale(matrix, axis=0)\n",
    "      matrix = matrix.T\n",
    "      for i, row in enumerate(matrix):\n",
    "         total += 1\n",
    "         correct += int(np.argmax(row) == i)\n",
    "      accs.append(correct/total * 100.0)\n",
    "      correct = 0\n",
    "      total = 0\n",
    "\n",
    "   brainsurfgnn_mse_models[label] = test_brainsurfcnn_mse_corr   \n",
    "   print('Max ACC:', CONTRASTS[np.argmax(accs)], np.max(accs))\n",
    "   print('Min ACC:', CONTRASTS[np.argmin(accs)], np.min(accs))\n",
    "   print('Avg ACC:',np.mean(accs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare all MSE models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "25 ICs Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_comparison = {}\n",
    "mse_comparison['BrainSurfCNN'] = brainsurf_mse_models['BrainSurfCNN: 100 ICS - MSE']\n",
    "mse_comparison['BrainSERF'] = brainserf_mse_models['BrainSERF: 25 ICS - MSE']\n",
    "mse_comparison['BrainSurfGCN'] = brainsurfgnn_mse_models['BrainSurfGNN: 100 ICS - MSE']\n",
    "mse_comparison['BaggedSurfCNN'] = brainsurf_cv_models['BaggedSurfCNN: 25 ICS - Bagged']\n",
    "mse_comparison['BrainSurfATN'] = brainsurfatn_mse_models['BrainSurfATN: 15 ICS - MSE']\n",
    "mse_comparison['Retest'] = retest['Retest Contrasts']\n",
    "\n",
    "plot_model_comparison_correlation(pred_by_model=mse_comparison, legend_size=18)\n",
    "plot_model_comparison_accuracy(pred_by_model=mse_comparison, indicies=paper_subset_indicies, legend_size=12)\n",
    "plot_model_comparison_accuracy(pred_by_model=mse_comparison)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BrainSurfCNN - FineTuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brainsurf_ft_models = {}\n",
    "predictions = []\n",
    "for ic in [15, 25, 50, 100]:\n",
    "   print('--------------------------------------------------')\n",
    "   label = f'BrainSurfCNN: {ic} ICS - Fine Tuned'\n",
    "   print(f'BrainSurfCNN: {ic} ICS - Fine Tuned')\n",
    "   mse_brainsurf_path = f\"../../aim3_results/HCP_feat64_s8_c{ic}_lr0.01_seed28_epochs50/finetuned_feat64_s8_c{ic}_lr0.01_seed28/predict_on_test_subj/best_corr/\"\n",
    "\n",
    "   multisample_brainsurfcnn_mse_pred = []\n",
    "   for i in range(len(test_subj_ids)):\n",
    "      subj = test_subj_ids[i]\n",
    "      pred_file = os.path.join(mse_brainsurf_path, \"%s_pred.npy\" % subj)\n",
    "      pred = np.load(pred_file)\n",
    "      multisample_brainsurfcnn_mse_pred.append(pred)\n",
    "\n",
    "   multisample_brainsurfcnn_mse_pred = np.asarray(multisample_brainsurfcnn_mse_pred)\n",
    "   brainsurfcnn_mse_pred = np.mean(multisample_brainsurfcnn_mse_pred, 1)\n",
    "   predictions.append(brainsurfcnn_mse_pred)\n",
    "\n",
    "   _, _, test_brainsurfcnn_mse_corr = compute_subj_contrast_corr(test_contrasts, brainsurfcnn_mse_pred, CONTRASTS, GROUP_CONTRAST_IDS, mask)\n",
    "\n",
    "   # plot_corr_matrices_across_contrasts(test_brainsurfcnn_mse_corr[0:3], CONTRASTS[0:3], vmin=0.35 , vmax=0.9)\n",
    "   # plt.show()\n",
    "   plot_corr_matrices_across_contrasts([test_brainsurfcnn_mse_corr[7], test_brainsurfcnn_mse_corr[4], test_brainsurfcnn_mse_corr[12]], [CONTRASTS[7], CONTRASTS[4], CONTRASTS[12]], vmin=0.35 , vmax=0.9)\n",
    "   plt.show()\n",
    "\n",
    "   paper_subset_mse_corr = [test_brainsurfcnn_mse_corr[idx] for idx in paper_subset_indicies]\n",
    "\n",
    "   total = 0\n",
    "   correct = 0\n",
    "   accs = []\n",
    "   for matrix, contrast in zip(test_brainsurfcnn_mse_corr, CONTRASTS):\n",
    "      matrix = scale(matrix, axis=0)\n",
    "      matrix = matrix.T\n",
    "      for i, row in enumerate(matrix):\n",
    "         total += 1\n",
    "         correct += int(np.argmax(row) == i)\n",
    "      accs.append(correct/total * 100.0)\n",
    "      correct = 0\n",
    "      total = 0\n",
    "\n",
    "   brainsurf_ft_models[label] = test_brainsurfcnn_mse_corr   \n",
    "   print('Max ACC:', CONTRASTS[np.argmax(accs)], np.max(accs))\n",
    "   print('Min ACC:', CONTRASTS[np.argmin(accs)], np.min(accs))\n",
    "   print('Avg ACC:',np.mean(accs))\n",
    "\n",
    "print('--------------------------------------------------')\n",
    "label = f'BrainSurfCNN: All ICS - Fine Tuned'\n",
    "print(label)\n",
    "brainsurfcnn_mse_pred = np.array(predictions).mean(0)\n",
    "_, _, test_brainsurfcnn_mse_corr = compute_subj_contrast_corr(test_contrasts, brainsurfcnn_mse_pred, CONTRASTS, GROUP_CONTRAST_IDS, mask)\n",
    "\n",
    "accs = []\n",
    "for matrix, contrast in zip(test_brainsurfcnn_mse_corr, CONTRASTS):\n",
    "   matrix = scale(matrix, axis=0)\n",
    "   matrix = matrix.T\n",
    "   for i, row in enumerate(matrix):\n",
    "      total += 1\n",
    "      correct += int(np.argmax(row) == i)\n",
    "   accs.append(correct/total * 100.0)\n",
    "   correct = 0\n",
    "   total = 0\n",
    "print('Max ACC:', CONTRASTS[np.argmax(accs)], np.max(accs))\n",
    "print('Min ACC:', CONTRASTS[np.argmin(accs)], np.min(accs))\n",
    "print('Avg ACC:',np.mean(accs))\n",
    "brainsurf_ft_models[label] = test_brainsurfcnn_mse_corr \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_comparison_correlation(pred_by_model=brainsurf_ft_models, legend_size=18)\n",
    "plot_model_comparison_accuracy(pred_by_model=brainsurf_ft_models, indicies=paper_subset_indicies, legend_size=12)\n",
    "plot_model_comparison_accuracy(pred_by_model=brainsurf_ft_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BrainSERF - FineTuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brainserf_ft_models = {}\n",
    "predictions = []\n",
    "for ic in [15, 25, 50]:\n",
    "   print('--------------------------------------------------')\n",
    "   label = f'BrainSERF: {ic} ICS - Fine Tuned'\n",
    "   print(label)\n",
    "   mse_brainsurf_path = f\"../../aim3_results/HCP_feat64_s8_c{ic}_lr0.01_seed28_epochs50/se_attn_finetuned_feat64_s8_c{ic}_lr0.01_seed28/predict_on_test_subj/best_corr/\"\n",
    "\n",
    "   multisample_brainsurfcnn_mse_pred = []\n",
    "   for i in range(len(test_subj_ids)):\n",
    "      subj = test_subj_ids[i]\n",
    "      pred_file = os.path.join(mse_brainsurf_path, \"%s_pred.npy\" % subj)\n",
    "      pred = np.load(pred_file)\n",
    "      multisample_brainsurfcnn_mse_pred.append(pred)\n",
    "\n",
    "   multisample_brainsurfcnn_mse_pred = np.asarray(multisample_brainsurfcnn_mse_pred)\n",
    "   brainsurfcnn_mse_pred = np.mean(multisample_brainsurfcnn_mse_pred, 1)\n",
    "   predictions.append(brainsurfcnn_mse_pred)\n",
    "\n",
    "   _, _, test_brainsurfcnn_mse_corr = compute_subj_contrast_corr(test_contrasts, brainsurfcnn_mse_pred, CONTRASTS, GROUP_CONTRAST_IDS, mask)\n",
    "   plot_corr_matrices_across_contrasts([test_brainsurfcnn_mse_corr[7], test_brainsurfcnn_mse_corr[4], test_brainsurfcnn_mse_corr[12]], [CONTRASTS[7], CONTRASTS[4], CONTRASTS[12]], vmin=0.35 , vmax=0.9)\n",
    "   plt.show()\n",
    "\n",
    "   paper_subset_mse_corr = [test_brainsurfcnn_mse_corr[idx] for idx in paper_subset_indicies]\n",
    "\n",
    "   total = 0\n",
    "   correct = 0\n",
    "   accs = []\n",
    "   for matrix, contrast in zip(test_brainsurfcnn_mse_corr, CONTRASTS):\n",
    "      matrix = scale(matrix, axis=0)\n",
    "      matrix = matrix.T\n",
    "      for i, row in enumerate(matrix):\n",
    "         total += 1\n",
    "         correct += int(np.argmax(row) == i)\n",
    "      accs.append(correct/total * 100.0)\n",
    "      correct = 0\n",
    "      total = 0\n",
    "\n",
    "   brainserf_ft_models[label] = test_brainsurfcnn_mse_corr   \n",
    "   print('Max ACC:', CONTRASTS[np.argmax(accs)], np.max(accs))\n",
    "   print('Min ACC:', CONTRASTS[np.argmin(accs)], np.min(accs))\n",
    "   print('Avg ACC:',np.mean(accs))\n",
    "print('--------------------------------------------------')\n",
    "label = f'BrainSERF: All ICS - Fine Tuned'\n",
    "print(label)\n",
    "brainsurfcnn_mse_pred = np.array(predictions).mean(0)\n",
    "_, _, test_brainsurfcnn_mse_corr = compute_subj_contrast_corr(test_contrasts, brainsurfcnn_mse_pred, CONTRASTS, GROUP_CONTRAST_IDS, mask)\n",
    "\n",
    "accs = []\n",
    "for matrix, contrast in zip(test_brainsurfcnn_mse_corr, CONTRASTS):\n",
    "   matrix = scale(matrix, axis=0)\n",
    "   matrix = matrix.T\n",
    "   for i, row in enumerate(matrix):\n",
    "      total += 1\n",
    "      correct += int(np.argmax(row) == i)\n",
    "   accs.append(correct/total * 100.0)\n",
    "   correct = 0\n",
    "   total = 0\n",
    "print('Max ACC:', CONTRASTS[np.argmax(accs)], np.max(accs))\n",
    "print('Min ACC:', CONTRASTS[np.argmin(accs)], np.min(accs))\n",
    "print('Avg ACC:',np.mean(accs))\n",
    "brainserf_ft_models[label] = test_brainsurfcnn_mse_corr  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_comparison_correlation(pred_by_model=brainserf_ft_models, legend_size=18)\n",
    "plot_model_comparison_accuracy(pred_by_model=brainserf_ft_models, indicies=paper_subset_indicies, legend_size=12)\n",
    "plot_model_comparison_accuracy(pred_by_model=brainserf_ft_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BrainSurfGCN - FineTuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brainsurfgnn_ft_models = {}\n",
    "predictions = []\n",
    "for ic in [15, 25, 50, 100]:\n",
    "   print('--------------------------------------------------')\n",
    "   label = f'BrainSurfGCN: {ic} ICS - Fine Tuned'\n",
    "   print(label)\n",
    "   mse_brainsurf_path = f\"../../aim3_results/HCP_feat64_s8_c{ic}_lr0.01_seed28_epochs50/gnn_finetuned_feat64_s8_c{ic}_lr0.01_seed28/predict_on_test_subj/best_corr/\"\n",
    "\n",
    "   multisample_brainsurfcnn_mse_pred = []\n",
    "   for i in range(len(test_subj_ids)):\n",
    "      subj = test_subj_ids[i]\n",
    "      pred_file = os.path.join(mse_brainsurf_path, \"%s_pred.npy\" % subj)\n",
    "      pred = np.load(pred_file)\n",
    "      multisample_brainsurfcnn_mse_pred.append(pred)\n",
    "\n",
    "   multisample_brainsurfcnn_mse_pred = np.asarray(multisample_brainsurfcnn_mse_pred)\n",
    "   brainsurfcnn_mse_pred = np.mean(multisample_brainsurfcnn_mse_pred, 1)\n",
    "   predictions.append(brainsurfcnn_mse_pred)\n",
    "\n",
    "   _, _, test_brainsurfcnn_mse_corr = compute_subj_contrast_corr(test_contrasts, brainsurfcnn_mse_pred, CONTRASTS, GROUP_CONTRAST_IDS, mask)\n",
    "\n",
    "   paper_subset_mse_corr = [test_brainsurfcnn_mse_corr[idx] for idx in paper_subset_indicies]\n",
    "\n",
    "   total = 0\n",
    "   correct = 0\n",
    "   accs = []\n",
    "   for matrix, contrast in zip(test_brainsurfcnn_mse_corr, CONTRASTS):\n",
    "   # for matrix, contrast in zip(paper_subset_mse_corr, paper_subset_contrasts):\n",
    "      # degree = np.linalg.\n",
    "      matrix = scale(matrix, axis=0)\n",
    "      # matrix = scale(matrix, axis=1)\n",
    "      matrix = matrix.T\n",
    "      for i, row in enumerate(matrix):\n",
    "         total += 1\n",
    "         correct += int(np.argmax(row) == i)\n",
    "      accs.append(correct/total * 100.0)\n",
    "      correct = 0\n",
    "      total = 0\n",
    "\n",
    "   brainsurfgnn_ft_models[label] = test_brainsurfcnn_mse_corr   \n",
    "   print('Max ACC:', CONTRASTS[np.argmax(accs)], np.max(accs))\n",
    "   print('Min ACC:', CONTRASTS[np.argmin(accs)], np.min(accs))\n",
    "   print('Avg ACC:',np.mean(accs))\n",
    "\n",
    "print('--------------------------------------------------')\n",
    "label = f'BrainSurfGCN: All ICS - Fine Tuned'\n",
    "print(label)\n",
    "brainsurfcnn_mse_pred = np.array(predictions).mean(0)\n",
    "_, _, test_brainsurfcnn_mse_corr = compute_subj_contrast_corr(test_contrasts, brainsurfcnn_mse_pred, CONTRASTS, GROUP_CONTRAST_IDS, mask)\n",
    "\n",
    "accs = []\n",
    "for matrix, contrast in zip(test_brainsurfcnn_mse_corr, CONTRASTS):\n",
    "   matrix = scale(matrix, axis=0)\n",
    "   matrix = matrix.T\n",
    "   for i, row in enumerate(matrix):\n",
    "      total += 1\n",
    "      correct += int(np.argmax(row) == i)\n",
    "   accs.append(correct/total * 100.0)\n",
    "   correct = 0\n",
    "   total = 0\n",
    "print('Max ACC:', CONTRASTS[np.argmax(accs)], np.max(accs))\n",
    "print('Min ACC:', CONTRASTS[np.argmin(accs)], np.min(accs))\n",
    "print('Avg ACC:',np.mean(accs))\n",
    "brainsurfgnn_ft_models[label] = test_brainsurfcnn_mse_corr   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_comparison_correlation(pred_by_model=brainsurfgnn_ft_models, legend_size=18)\n",
    "plot_model_comparison_accuracy(pred_by_model=brainsurfgnn_ft_models, indicies=paper_subset_indicies, legend_size=12)\n",
    "plot_model_comparison_accuracy(pred_by_model=brainsurfgnn_ft_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retest Contrasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retest = {}\n",
    "for ic in [1]:\n",
    "   print('--------------------------------------------------')\n",
    "   label = f'Retest Contrasts'\n",
    "   print(label)\n",
    "   mse_brainsurf_path = f\"../../data/retest_contrasts/contrasts/\"\n",
    "\n",
    "   multisample_brainsurfcnn_mse_pred = []\n",
    "   for i in range(len(test_subj_ids)):\n",
    "      subj = test_subj_ids[i]\n",
    "      pred_file = os.path.join(mse_brainsurf_path, \"%s_joint_LR_task_contrasts.npy\" % subj)\n",
    "      pred = np.load(pred_file)\n",
    "      multisample_brainsurfcnn_mse_pred.append(pred)\n",
    "\n",
    "   brainsurfcnn_mse_pred = np.asarray(multisample_brainsurfcnn_mse_pred)\n",
    "   \n",
    "   _, _, test_brainsurfcnn_mse_corr = compute_subj_contrast_corr(test_contrasts, brainsurfcnn_mse_pred, CONTRASTS, GROUP_CONTRAST_IDS, mask)\n",
    "\n",
    "   plot_corr_matrices_across_contrasts(test_brainsurfcnn_mse_corr[0:3], CONTRASTS[0:3], vmin=0.35 , vmax=0.9)\n",
    "   plt.show()\n",
    "   plot_corr_matrices_across_contrasts([test_brainsurfcnn_mse_corr[7], test_brainsurfcnn_mse_corr[4], test_brainsurfcnn_mse_corr[12]], [CONTRASTS[7], CONTRASTS[4], CONTRASTS[12]], vmin=0.35 , vmax=0.9)\n",
    "   plt.show()\n",
    "\n",
    "   paper_subset_mse_corr = [test_brainsurfcnn_mse_corr[idx] for idx in paper_subset_indicies]\n",
    "\n",
    "   total = 0\n",
    "   correct = 0\n",
    "   accs = []\n",
    "   for matrix, contrast in zip(paper_subset_mse_corr, paper_subset_contrasts):\n",
    "      matrix = scale(matrix, axis=0)\n",
    "      matrix = matrix.T\n",
    "      for i, row in enumerate(matrix):\n",
    "         total += 1\n",
    "         correct += int(np.argmax(row) == i)\n",
    "      accs.append(correct/total * 100.0)\n",
    "      correct = 0\n",
    "      total = 0\n",
    "\n",
    "   retest[label] = test_brainsurfcnn_mse_corr   \n",
    "   print('Max ACC:', CONTRASTS[np.argmax(accs)], np.max(accs))\n",
    "   print('Min ACC:', CONTRASTS[np.argmin(accs)], np.min(accs))\n",
    "   print('Avg ACC:',np.mean(accs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Models - Fine Tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_comparison = {}\n",
    "ft_comparison['BrainSurfCNN'] = brainsurf_ft_models['BrainSurfCNN: 25 ICS - Fine Tuned']\n",
    "ft_comparison['BrainSERF (ours)'] = brainserf_ft_models['BrainSERF: 25 ICS - Fine Tuned']\n",
    "ft_comparison['BrainSurfGCN (ours)'] = brainsurfgnn_ft_models['BrainSurfGCN: 50 ICS - Fine Tuned']\n",
    "ft_comparison['Retest'] = retest['Retest Contrasts']\n",
    "\n",
    "\n",
    "plot_model_comparison_correlation(pred_by_model=ft_comparison, legend_size=18)\n",
    "plot_model_comparison_correlation(pred_by_model=ft_comparison, legend_size=18, metric='Correlation Difference', ymin=-0.2, ymax=0.51)\n",
    "plot_model_comparison_accuracy(pred_by_model=ft_comparison, indicies=paper_subset_indicies, legend_size=12)\n",
    "plot_model_comparison_accuracy(pred_by_model=ft_comparison)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
